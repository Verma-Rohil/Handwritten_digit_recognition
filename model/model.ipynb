{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd59726-2a32-4206-a8b5-dc055c11108e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329cd7a-104c-4eeb-8992-69b101f63dcc",
   "metadata": {},
   "source": [
    "**Name**: <font color=\"lime\">**Panghal vishesh**</font>  \n",
    "**Specialization**: Machine Learning and Computer Vision  \n",
    "**Date**: Dec/29/2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4fa64-522e-4545-8762-4fc3e0b83de5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d149b-2731-4796-8899-c9a44a6f032e",
   "metadata": {},
   "source": [
    "- # ðŸ¤– ANN vs ðŸ§  CNN: A Comprehensive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a170d-0ae7-48d8-adeb-68a00fd4118f",
   "metadata": {},
   "source": [
    "***\n",
    "<div style=\"background-color: transparent; padding: 10px;\">\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px; text-align: center;\">\n",
    "      <img src=\"https://media.licdn.com/dms/image/v2/D4D12AQGBG0RgeyMHVw/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1692187763032?e=2147483647&v=beta&t=f139Om73qZxbbvCtA-EKhPWUC-YU3qCVRmNCqDM2d3g\" alt=\"ANN\" style=\"width: 600px; height: auto;background-color:white;\">\n",
    "    </td>\n",
    "    <td style=\"padding: 10px; text-align: center;\">\n",
    "      <img src=\"https://media.licdn.com/dms/image/D5612AQGOui8XZUZJSA/article-cover_image-shrink_720_1280/0/1680532048475?e=2147483647&v=beta&t=8aodfukDSrrnnxOVSNobKYJtbtSDB7yC83LUky-Ob68\" alt=\"CNN\" style=\"width: 800px; height: auto;\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53b87b-f064-43b6-b341-17b92064ca3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <font color=gold> ANN<font/>  \n",
    "- An Artificial Neural Network in the field of Artificial intelligence where it attempts to mimic the network of neurons makes up a human brain so that computers will have an option to understand things and make decisions in a human-like manner. The artificial neural network is designed by programming computers to behave simply like interconnected brain cells.\n",
    "\n",
    "## <font color='white'> Key Components of an Artificial Neural Network (ANN) <font/>  \n",
    "1. **Input Layer**:\n",
    "   - The input layer takes in the data features (e.g., numbers, vectors) and passes them to the next layer. Each neuron represents one feature of the input data.\n",
    "2. **Hidden Layers**:\n",
    "   - These layers consist of neurons that process the input data. Neurons in each layer are connected to every neuron in the previous and next layers (fully connected). The hidden layers perform computations and learn the relationships between input features through activation functions.\n",
    "3. **Output Layer**:\n",
    "   - The output layer produces the final result or prediction (e.g., class labels in classification tasks or a continuous value in regression). The number of neurons in this layer corresponds to the number of output classes or target values.\n",
    "4. **Weights and Biases**:\n",
    "   - Weights determine the strength of the connection between neurons, and biases allow the model to shift the activation function to better fit the data. Both are learned during the training process.\n",
    "5. **Activation Functions**:\n",
    "   - Activation functions introduce non-linearity into the network, enabling it to learn complex patterns. Common functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "6. **Loss Function**:\n",
    "   - The loss function measures how far the model's predictions are from the actual target values. During training, the network tries to minimize this loss to improve its performance. Examples include Mean Squared Error (MSE) and Cross-Entropy Loss.\n",
    "7. **Optimizer**:\n",
    "   - Optimizers update the weights and biases during training to minimize the loss function. Examples include Stochastic Gradient Descent (SGD), Adam, and RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1286a76-b587-494c-bd2d-a412ae66f3af",
   "metadata": {},
   "source": [
    "## <font color=gold> CNN<font/>  \n",
    "- An Convolutional Neural Network is a type of deep learning algorithm that is particularly well-suited for image recognition and processing tasks. It is made up of multiple layers, including convolutional layers, pooling layers, and fully connected layers.\n",
    "\n",
    "## <font color='white'> Key Components of an Convolutional Neural Network (CNN) <font/> \n",
    "1. **Convolutional Layers**:\n",
    "   - The core component of a CNN, where filters (kernels) slide over the input image (or feature maps) to detect local patterns like edges, textures, and shapes. The convolution operation applies a filter to the input to produce a feature map.\n",
    "2. **ReLU (Rectified Linear Unit)**:\n",
    "   - After convolution, the ReLU activation function is typically applied to introduce non-linearity. It replaces all negative values in the feature map with zero, allowing the network to learn complex patterns.\n",
    "3. **Pooling Layers**:\n",
    "   - Pooling reduces the spatial dimensions (height and width) of the feature maps, making the network more computationally efficient and helping to avoid overfitting. The most common type is Max Pooling, which selects the maximum value from a small region of the feature map.\n",
    "4. **Fully Connected Layer**:\n",
    "   - After several convolutional and pooling layers, the high-level features are passed to one or more fully connected layers (like those in a regular ANN). These layers are used to make final predictions, such as class labels or regression outputs.\n",
    "5. **Dropout Layer**:\n",
    "   - A regularization technique where random neurons are \"dropped\" during training to prevent overfitting. This forces the network to generalize better.\n",
    "6. **Batch Normalization**:\n",
    "   - A technique to normalize the activations of each layer in the network to improve training speed and stability.\n",
    "7. **Softmax / Sigmoid (Output Layer)**:\n",
    "   - For classification tasks, the output layer often uses a Softmax activation for multi-class classification (converting raw scores into probabilities).\n",
    "   - For binary classification, Sigmoid is often used.\n",
    "8. **Loss Function and Optimizer**:\n",
    "   - Similar to ANN, CNNs use loss functions like Cross-Entropy Loss for classification tasks and an optimizer like Adam to minimize the loss during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bda77-7716-441c-98d5-9586d6705501",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border: 1px solid black; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 10px; text-align: center;\">Aspect</th>\n",
    "    <th style=\"padding: 10px; text-align: center;\">ANN</th>\n",
    "    <th style=\"padding: 10px; text-align: center;\">CNN</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Architecture</td>\n",
    "    <td style=\"padding: 10px;\">Fully connected layers</td>\n",
    "    <td style=\"padding: 10px;\">Convolutional, pooling layers</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Input</td>\n",
    "    <td style=\"padding: 10px;\">1D data (tabular, vector)</td>\n",
    "    <td style=\"padding: 10px;\">2D/3D data (images, videos)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Feature Extraction</td>\n",
    "    <td style=\"padding: 10px;\">Manual</td>\n",
    "    <td style=\"padding: 10px;\">Automatic via convolutions</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Parameter Sharing</td>\n",
    "    <td style=\"padding: 10px;\">No</td>\n",
    "    <td style=\"padding: 10px;\">Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Spatial Hierarchy</td>\n",
    "    <td style=\"padding: 10px;\">No</td>\n",
    "    <td style=\"padding: 10px;\">Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Use Cases</td>\n",
    "    <td style=\"padding: 10px;\">General tasks</td>\n",
    "    <td style=\"padding: 10px;\">Image, video, spatial data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Efficiency</td>\n",
    "    <td style=\"padding: 10px;\">Lower for high-dimensional</td>\n",
    "    <td style=\"padding: 10px;\">Higher for high-dimensional</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42c58e-cc2a-40bf-94e5-9e836708181d",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">Handwritten Digit Recognition (ANN)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46553e0-93b3-4bf2-99b6-bd4e085358c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0951c786-5946-4aee-8e46-8a09a2bf6a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1d9f5-41c6-47e7-bb7e-b71dc5923a7d",
   "metadata": {},
   "source": [
    "## <font color='gold'> Prepare Dataset<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98452816-19cd-4723-a796-4db810bb0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((28, 28)),                 # Resize to 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))         # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315da368-86f1-4c42-a03a-37b7b6043948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 31715\n",
      "Class to Index Mapping: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageFolder(\"../dataset/\",transform=transform)\n",
    "\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"Class to Index Mapping: {dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b9c3c16-49da-48cf-b642-c306e04e6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdbd007-bd39-4f36-b6f3-ca5e603b4d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 25372\n",
      "Testing Samples: 6343\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8*len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset,[train_size,test_size])\n",
    "\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Testing Samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923cdc14-345c-4884-b747-5321ffdb3be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Loader\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False)\n",
    "\n",
    "classes = dataset.classes\n",
    "for images,labels in train_loader:\n",
    "    for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title(f\"Label: {classes[labels[i]]}\")\n",
    "        plt.axis('off')\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7be416-735a-478a-b52d-ba36691e5f8e",
   "metadata": {},
   "source": [
    "## <font color='gold'> Define Model<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b0f63df-c5b0-4690-8b50-934cc3935ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.network(features)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f40219-9d9b-4afe-a876-4b83d9de48e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label = train_dataset[0]\n",
    "height,width = image.shape[1],image.shape[2]\n",
    "num_features = height*width\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55fcd1f-c81e-4ad2-8844-1e440642ebcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNModel(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN_model = ANNModel(num_features)\n",
    "ANN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76f4a1-3e28-4417-bcfa-74de6f9bc17f",
   "metadata": {},
   "source": [
    "## <font color='gold'> Train Model <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e735f1-7f31-49b2-883c-998da11dd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b8c16c-008c-4a20-883e-b87378bd76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ANN_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a26f0521-10ac-41ab-a186-7278cc7f75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9053, Accuracy: 70.61%\n",
      "Epoch [2/10], Loss: 0.4864, Accuracy: 84.24%\n",
      "Epoch [3/10], Loss: 0.3605, Accuracy: 88.05%\n",
      "Epoch [4/10], Loss: 0.2958, Accuracy: 90.18%\n",
      "Epoch [5/10], Loss: 0.2559, Accuracy: 91.53%\n",
      "Epoch [6/10], Loss: 0.2151, Accuracy: 92.81%\n",
      "Epoch [7/10], Loss: 0.1994, Accuracy: 93.35%\n",
      "Epoch [8/10], Loss: 0.1876, Accuracy: 93.52%\n",
      "Epoch [9/10], Loss: 0.1755, Accuracy: 94.03%\n",
      "Epoch [10/10], Loss: 0.1590, Accuracy: 94.47%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    ANN_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten images (MNIST images are 28x28)\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = ANN_model(images)\n",
    "\n",
    "        loss = loss_function(output,labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss +=loss.item()\n",
    "        _,predicted = torch.max(output,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d47b8ba1-35a0-4d3a-be95-923c929833fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.72%\n"
     ]
    }
   ],
   "source": [
    "ANN_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = ANN_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fff0d0d-8071-469b-836c-8ef61cce9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35de425c-0e7d-4fdd-9bfe-098994685d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]         100,480\n",
      "       BatchNorm1d-2                  [-1, 128]             256\n",
      "              ReLU-3                  [-1, 128]               0\n",
      "            Linear-4                   [-1, 64]           8,256\n",
      "       BatchNorm1d-5                   [-1, 64]             128\n",
      "              ReLU-6                   [-1, 64]               0\n",
      "            Linear-7                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 109,770\n",
      "Trainable params: 109,770\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model=ANN_model, input_size=(28*28,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de96b97-f4dc-43ff-b183-061b5b665a2d",
   "metadata": {},
   "source": [
    "## <font color='gold'> Export model<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf57173-6fba-474b-b8b1-c17ef8562945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(ANN_model.state_dict(), '../model/(ANN)digit_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a85bd-03b6-465a-8e39-14aea8244407",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460348b-06b6-4ea2-957e-c44c838b18a0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634731a-7c0b-488b-a636-a16bce49071f",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">Handwritten Digit Recognition (CNN)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9464fcd-82f8-4019-b12e-2383f036fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267729fc-abd5-498c-8039-2d0a3625c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "Training Samples: 25372\n",
      "Testing Samples: 6343\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='../dataset/',transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset)) \n",
    "test_size = len(train_dataset) - train_size \n",
    "\n",
    "train_subset, test_subset = random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "# Train loader\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Test loader\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break\n",
    "\n",
    "print(f\"Training Samples: {len(train_subset)}\")\n",
    "print(f\"Testing Samples: {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0371c54f-a925-442f-9e6d-c91adb00e3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    plt.imshow(images[0].squeeze(), cmap='grey')\n",
    "    plt.title(f\"Label: {labels[0]}\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c470b6-f601-415f-a409-20cafbd2e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f042a3fb-774a-4935-8e12-3a51f34f5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = None  # Will initialize dynamically\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        feature = F.relu(self.conv1(feature))\n",
    "        feature = self.pool(feature)\n",
    "        feature = F.relu(self.conv2(feature))\n",
    "        feature = self.pool(feature)\n",
    "        feature = feature.view(feature.size(0), -1)  # Flatten the feature map\n",
    "        if not self.fc1:  # Initialize fc1 dynamically based on flattened feature size\n",
    "            self.fc1 = nn.Linear(feature.size(1), 128)\n",
    "        feature = F.relu(self.fc1(feature))\n",
    "        feature = self.fc2(feature)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc4c58b-12ca-422b-b156-7df062eccdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model = CNNModel()\n",
    "CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6fbbb67-1e97-4dd8-a0bd-1a83b76918a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNN_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca71d65-ae48-4b1b-a5d5-9670fc93352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.7891, Accuracy: 74.92%\n",
      "Epoch [2/25], Loss: 0.3808, Accuracy: 88.24%\n",
      "Epoch [3/25], Loss: 0.2986, Accuracy: 90.79%\n",
      "Epoch [4/25], Loss: 0.2619, Accuracy: 91.73%\n",
      "Epoch [5/25], Loss: 0.2266, Accuracy: 93.11%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\dataScience\\handwritten_digit_recognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\dataScience\\handwritten_digit_recognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\dataScience\\handwritten_digit_recognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\dataScience\\handwritten_digit_recognition\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\dataScience\\handwritten_digit_recognition\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\dataScience\\handwritten_digit_recognition\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\dataScience\\handwritten_digit_recognition\\Lib\\site-packages\\torchvision\\datasets\\folder.py:262\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CNN_model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    CNN_model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = CNN_model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2a5c7aa-c47e-46c3-a627-b6182faa9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.35%\n"
     ]
    }
   ],
   "source": [
    "CNN_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = CNN_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b38c1ce-c7bf-4338-801e-1508b7afeebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "         MaxPool2d-2           [-1, 32, 14, 14]               0\n",
      "            Conv2d-3           [-1, 64, 14, 14]          18,496\n",
      "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
      "            Linear-5                  [-1, 128]         401,536\n",
      "            Linear-6                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 421,642\n",
      "Trainable params: 421,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 1.61\n",
      "Estimated Total Size (MB): 1.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model=CNN_model, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92441727-368b-447a-a8c1-35c90ba41fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNN_model.state_dict(), '../model/(CNN)digit_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fded686-d85a-4ae9-b452-31be0a461cae",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab77edf-2aa8-4f76-99e3-96648590fbe3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a80c1-676f-486f-8b66-7a4517ac4e30",
   "metadata": {},
   "source": [
    "## <font color=gold> Observation: <font/>\n",
    "1. **Model Performance and Insights**:\n",
    "   - The **ANN model** achieved a training accuracy of **98.02%** and a test accuracy of **91.71**\n",
    "   - The **CNN model** achieved a training accuracy of **97.24%** and a test accuracy of **92.35%**, which highlights its ability to generalize well to unseen data from the same distribution.\n",
    "\n",
    "2. **Performance on Scanned vs. Real-World Images**:\n",
    "   - ANN:\n",
    "     - The model performed exceptionally well on **scanned images**, with predictions closely matching ground truth,but not on Real-World Images.\n",
    "    - CNN:\n",
    "      - The model performed exceptionally well on **scanned images**,as well as on **real-world images** with predictions closely matching ground truth.\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (handwritten_digit_recognition)",
   "language": "python",
   "name": "handwritten_digit_recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
